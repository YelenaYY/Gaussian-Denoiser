{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d448c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "270ab56f-9f2f-4b94-ba79-64c571d1caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data.dataset import Dataset  # noqa:E402\n",
    "from torchvision.io import decode_image  # noqa:E402\n",
    "from torchvision import transforms  # noqa:E402\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path  # noqa:E402\n",
    "from unit_test import TestAssertions, unit_test, run_unit_tests  # noqa:E402\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f965cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"checkpoints\")\n",
    "MAX_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "05986864-da93-41d6-879f-4ff77770832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda device is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "use_cuda = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf97a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded and extracted to data/\n"
     ]
    }
   ],
   "source": [
    "!bash download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d748799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 321, 481])\n"
     ]
    }
   ],
   "source": [
    "# read any image from the data/BSDS300/image/train folder\n",
    "image_path = Path(\"data/BSDS300/images/train/2092.jpg\")\n",
    "image = decode_image(str(image_path))\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469c361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfa18854-615e-464b-b0e0-a9e2b83f928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 unit tests\n",
      "----------------------------------------\n",
      "Running test: dataset_loading_and_indexing\n",
      "✓ dataset_loading_and_indexing passed\n",
      "Running test: dataset_transform_cropping\n",
      "✓ dataset_transform_cropping passed\n",
      "Running test: dataset_transform_patch_cropping\n",
      "✓ dataset_transform_patch_cropping passed\n",
      "Running test: read_all_patches\n",
      "torch.Size([204800, 1, 40, 40])\n",
      "✓ read_all_patches passed\n",
      "----------------------------------------\n",
      "Tests run: 4, Passed: 4, Failed: 0\n"
     ]
    }
   ],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class ExtractPatch():\n",
    "    @abstractmethod\n",
    "    def extract_patches(self, idx, patch_size, stride, batch_size=128):\n",
    "        pass\n",
    "\n",
    "class Bsd300(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = Path(data_folder)\n",
    "        self.transform = transform\n",
    "        self.labels = []\n",
    "        with open(self.data_folder / \"iids_train.txt\") as f:\n",
    "            self.labels = f.readlines()\n",
    "        if len(self.labels) == 0:\n",
    "            print(\"No labels found in\", data_folder)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img_name = self.data_folder / f\"images/train/{label.strip()}.jpg\"\n",
    "        image = decode_image(str(img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class Train400(Dataset, ExtractPatch):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = Path(data_folder)\n",
    "        self.transform = transform\n",
    "        # list get file stems in the data_folder/train400 folder\n",
    "        self.labels = [f.stem for f in self.data_folder.glob(\"*.png\")]\n",
    "        if len(self.labels) == 0:\n",
    "            raise FileNotFoundError(f\"No images found in {data_folder}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img_name = self.data_folder / f\"{label}.png\"\n",
    "        image = decode_image(str(img_name))\n",
    "        return image, label\n",
    "    \n",
    "    def extract_patches(self, idx, patch_size, stride, batch_size):\n",
    "        image, label = self.__getitem__(idx)\n",
    "        patches = []\n",
    "        for i in range(0, image.shape[1] - patch_size + 1, stride):\n",
    "            for j in range(0, image.shape[2] - patch_size + 1, stride):\n",
    "                patch = image[:,i:i+patch_size, j:j+patch_size]\n",
    "                if self.transform:\n",
    "                    patch = self.transform(patch)\n",
    "                patches.append(patch)\n",
    "        \n",
    "        n_patches_to_remove_for_batch_normalization = len(patches) % batch_size\n",
    "        patches = patches[:-n_patches_to_remove_for_batch_normalization]\n",
    "        patches = torch.stack(patches)\n",
    "        return patches, label\n",
    "    \n",
    "    def read_all_patches(self, patch_size, stride, batch_size=128):\n",
    "        all_patches = []\n",
    "        for i in range(len(self)):\n",
    "            patches, _ = self.extract_patches(i, patch_size, stride, batch_size)\n",
    "            all_patches.append(patches)\n",
    "        return torch.cat(all_patches)\n",
    "\n",
    "\n",
    "class NoisyPatchDataset(Dataset):\n",
    "    def __init__(self, patches, sigma: float | tuple[float, float]=25):\n",
    "        self.patches = patches\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.patches.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patches = self.patches[idx]\n",
    "        if isinstance(self.sigma, tuple): # sigma is a tuple of two floats\n",
    "            sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "            noisy_patches = patches + torch.randn_like(patches) * sigma\n",
    "        elif isinstance(self.sigma, float): # sigma is a float\n",
    "            noisy_patches = patches + torch.randn_like(patches) * self.sigma\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid noise sigma type: {type(self.sigma)}\")\n",
    "\n",
    "        return noisy_patches, patches\n",
    "\n",
    "class Tests:\n",
    "    @staticmethod\n",
    "    @unit_test\n",
    "    def dataset_loading_and_indexing():\n",
    "        dataset = Bsd300(\"data/BSDS300\")\n",
    "        TestAssertions.assert_eq(len(dataset), 200, \"Dataset should have = 200 images\")\n",
    "        _, _ = dataset[0] \n",
    "        dataset = Train400(\"data/Train400\")\n",
    "        TestAssertions.assert_eq(len(dataset), 400, \"Dataset should have = 400 images\")\n",
    "        _, _ = dataset[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    @unit_test\n",
    "    def dataset_transform_cropping():\n",
    "        dataset = Bsd300(\"data/BSDS300\", transform=transforms.RandomCrop(180))\n",
    "        for i in range(len(dataset)):\n",
    "            image, _ = dataset[i]\n",
    "            TestAssertions.assert_eq(image.shape[1], 180, \"Image H should be cropped to 180\")\n",
    "            TestAssertions.assert_eq(image.shape[2], 180, \"Image W should be cropped to 180\")\n",
    "    \n",
    "    @staticmethod\n",
    "    @unit_test\n",
    "    def dataset_transform_patch_cropping():\n",
    "        transform = transforms.RandomChoice([\n",
    "            transforms.RandomRotation((90, 90)),\n",
    "        ])\n",
    "        dataset = Train400(\"data/Train400\", transform=transform)\n",
    "        stride = 6\n",
    "        patches, label = dataset.extract_patches(0, patch_size=40, stride=stride, batch_size=128)\n",
    "        patch1 = patches[0,:,:,:]\n",
    "\n",
    "        dataset = Train400(\"data/Train400\")\n",
    "        stride = 6\n",
    "        patches, label = dataset.extract_patches(0, patch_size=40, stride=stride, batch_size=128)\n",
    "        patch2 = patches[0,:,:,:]\n",
    "        patch2 = torch.rot90(patch2, dims=(1, 2))\n",
    "        # check if patch1 and patch2 are the same use torch.allclose\n",
    "        TestAssertions.assert_eq(torch.allclose(patch1, patch2), True, f\"Patch 1 and Patch 2 should be the same, {patch1.shape}, {patch2.shape}\")\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @unit_test\n",
    "    def read_all_patches():\n",
    "        dataset = Train400(\"data/Train400\")\n",
    "        patches = dataset.read_all_patches(patch_size=40, stride=6)\n",
    "        print(patches.shape)\n",
    "        noisy_dataset = NoisyPatchDataset(patches, sigma=(0, 50))\n",
    "        TestAssertions.assert_eq(len(noisy_dataset), 204800, \"Noisy dataset should have 204800 patches\")\n",
    "\n",
    "run_unit_tests(Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee542929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, num_layers=17, num_channels=64, kernel_size=3, image_channels=1, padding=1):\n",
    "        super(DnCNN, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels=image_channels, out_channels=num_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(num_layers-2):\n",
    "            layers.append(nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(num_channels, eps=0.0001, momentum = 0.95))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(in_channels=num_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                init.orthogonal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dncnn(x)\n",
    "\n",
    "def save_checkpoint(model, epoch, save_dir: Path):\n",
    "    if not save_dir.exists():\n",
    "        save_dir.mkdir(parents=True)\n",
    "    torch.save(model.state_dict(), save_dir / f'model_{epoch:03d}.pth')\n",
    "\n",
    "def load_checkpoint(model, save_dir):\n",
    "    checkpoints = list(save_dir.glob('model_*.pth'))\n",
    "    if len(checkpoints) == 0:\n",
    "        print(\"No checkpoints found, train from beginning\")\n",
    "        return\n",
    "    # sort the checkpoints by the epoch number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.stem.split('_')[-1]), reverse=True)\n",
    "\n",
    "    # get the latest checkpoint\n",
    "    latest_checkpoint = checkpoints[0]\n",
    "    model.load_state_dict(torch.load(latest_checkpoint))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, train from beginning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m output = noisy_patches - model(noisy_patches)\n\u001b[32m     32\u001b[39m loss = criterion(output, patches)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m epoch_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m loss.backward()\n\u001b[32m     36\u001b[39m optimizer.step()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = DnCNN()\n",
    "load_checkpoint(model, SAVE_DIR)\n",
    "model.train()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss(reduction = 'sum') \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.2)  # learning rates\n",
    "\n",
    "training_dataset = Train400(\"data/Train400\")\n",
    "patches = training_dataset.read_all_patches(patch_size=40, stride=6, batch_size=128)\n",
    "# convert patches to float32 and normalize to [0, 1]\n",
    "patches = patches.to(torch.float32) / 255.0\n",
    "noisy_dataset = NoisyPatchDataset(patches, sigma=(0, 50))\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    dataloader = DataLoader(noisy_dataset, batch_size=128, shuffle=True)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Create progress bar for batches within this epoch\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{MAX_EPOCH}', leave=False)\n",
    "    \n",
    "    for batch_idx, (noisy_patches, patches) in enumerate(pbar):\n",
    "        if use_cuda:\n",
    "            noisy_patches = noisy_patches.cuda()\n",
    "            patches = patches.cuda()\n",
    "\n",
    "        output = model(noisy_patches) # learned noise\n",
    "        target = noisy_patches - patches # original noise\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        avg_loss = epoch_loss / (batch_idx + 1)\n",
    "        pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'Avg Loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "    scheduler.step()\n",
    "    save_checkpoint(model, epoch, SAVE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
